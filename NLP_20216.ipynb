{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXmYMD8kdcbk"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.27.8\n",
        "!pip install tiktoken\n",
        "!pip install langchain\n",
        "!pip install hnswlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "4mlS7KLmfSOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import openai\n",
        "import hnswlib\n",
        "import langchain\n",
        "from langchain.text_splitter import TextSplitter, CharacterTextSplitter\n",
        "import PyPDF2\n",
        "import requests"
      ],
      "metadata": {
        "id": "pRzHXSMCdjcz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = \"\"\n",
        "openai_params = {\"model\":\"gpt-4-1106-preview\",\n",
        "                 \"temperature\":0.5,\n",
        "                 \"frequency_penalty\":0.0,\n",
        "                 \"presence_penalty\":0.0,\n",
        "                 \"max_tokens\":1500,\n",
        "                 \"top_p\":1}\n"
      ],
      "metadata": {
        "id": "lp2K0cSFgqWQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(prompt,openai_params):\n",
        "  message = [{\"role\":\"user\",\"content\":prompt}]\n",
        "  response = openai.ChatCompletion.create(messages=message,\n",
        "                                        **openai_params)\n",
        "\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "h3B55W2FduAa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text_gpt(content,chunk_size=120,splitter_pattern=\"\"):\n",
        "    \"\"\"\n",
        "    Tokenize the text according to openai tokenizer using Langchain\n",
        "    :param content:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if not splitter_pattern:\n",
        "\n",
        "        if \"\\n\\n\" in content:\n",
        "\n",
        "            text_splitter_ = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=chunk_size, chunk_overlap=0,encoding_name=\"cl100k_base\")\n",
        "        elif \"\\n\" in content:\n",
        "            text_splitter_ = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=chunk_size, chunk_overlap=0,\n",
        "                                                                         separator=\"\\n\",encoding_name=\"cl100k_base\")\n",
        "        else:\n",
        "            text_splitter_ = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=chunk_size, chunk_overlap=0,\n",
        "                                                                         separator=\" \",encoding_name=\"cl100k_base\")\n",
        "    else:\n",
        "        text_splitter_ = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=chunk_size,chunk_overlap=0,\n",
        "                                                                    separator=splitter_pattern,encoding_name=\"cl100k_base\")\n",
        "    passages = text_splitter_.split_text(content)\n",
        "\n",
        "    return passages\n",
        "\n",
        "\n",
        "# tokenized_text = tokenize_text_by_page(text)\n",
        "# for page, tokens in tokenized_text.items():\n",
        "#     print(f\"Page {page}: {tokens}\\n\")"
      ],
      "metadata": {
        "id": "xbKiim4mduDA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_file_path):\n",
        "    # Open the PDF file\n",
        "    with open(pdf_file_path, 'rb') as file:\n",
        "        # Create PDF reader object\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        extracted_text = \"\"\n",
        "\n",
        "        for page in pdf_reader.pages:\n",
        "\n",
        "            extracted_text += page.extract_text()\n",
        "\n",
        "        return extracted_text"
      ],
      "metadata": {
        "id": "g-iQkdqjd309"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_index2(text_chunks):\n",
        "\n",
        "    embeddings = get_embedding_list(text_chunks)\n",
        "    if len(embeddings) == 0:\n",
        "        print(\"No embeddings generated.\")\n",
        "        return None, {}\n",
        "\n",
        "    dimension = len(embeddings[0])  # Dynamically get the dimension of embeddings\n",
        "\n",
        "    index1 = hnswlib.Index(space='l2', dim=dimension)\n",
        "    index1.init_index(max_elements=len(text_chunks), ef_construction=200, M=16)\n",
        "\n",
        "    # Bulk adding to the index\n",
        "    index1.add_items(embeddings)\n",
        "\n",
        "    index1.set_ef(50)\n",
        "    return index1"
      ],
      "metadata": {
        "id": "yhO7VBmPelgK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding_list(texts, model=\"text-embedding-ada-002\"):\n",
        "  texts = [re.sub(\"\\n+\", \" \", text) for text in texts]\n",
        "  embedding_data = openai.Embedding.create(input = texts, model=model)['data']\n",
        "  print(\"embeddings returned from openai\")\n",
        "  return [embedding_data[i][\"embedding\"] for i in range(len(embedding_data))]\n",
        "\n",
        "\n",
        "\n",
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "  text = re.sub(\"\\n+\", \" \", text)\n",
        "  return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']"
      ],
      "metadata": {
        "id": "CSzr3jfFev0f"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_similar_text2(query, index, top_k):\n",
        "    query_vector = get_embedding(query)\n",
        "    try:\n",
        "        labels, distances = index.knn_query(query_vector, k=top_k)\n",
        "\n",
        "        # Flatten the labels and distances since we have a single query\n",
        "        labels = labels.flatten()\n",
        "        distances = distances.flatten()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        results = []\n",
        "    return labels"
      ],
      "metadata": {
        "id": "OcnJWj_jeljA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_dict = extract_text_from_pdf(\"/content/230_lipnet_end_to_end_sentence_lev.pdf\")\n",
        "chunks = tokenize_text_gpt(text_dict)"
      ],
      "metadata": {
        "id": "VQjUVeadfMxR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfOaBLLWfuOR",
        "outputId": "d72a998b-440f-4ade-97bb-1d765a150cd5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "119"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "index = create_index2(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o97icW6Peq6g",
        "outputId": "8b88eb06-e826-4cf0-ff5f-f8bfb6e4d341"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embeddings returned from openai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the lipreading datasets used?\""
      ],
      "metadata": {
        "id": "Bo5n0xDqeq9Y"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "searched_index = search_similar_text2(query,index,10)"
      ],
      "metadata": {
        "id": "k6G40p8Meq_k"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\"\n",
        "for i in searched_index:\n",
        "  context = context + \" \" +chunks[i] + \"\\n\\n\""
      ],
      "metadata": {
        "id": "rYcqJDUlerB1"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def qa_prompt(instruction, context):\n",
        "    _prompt = f\"\"\"When responding to instructions, always ensure your answers:\n",
        "1.Condense essential information from provided documents to directly tackle the task.\n",
        "2.Analyze and address any disparities or inaccuracies between the instruction and document content.\n",
        "3.Highlight crucial details, eliminating extraneous information for clarity.\n",
        "4.Recognize any missing content, proposing alternatives or additional sources if needed.\n",
        "5.Uphold ethical standards by maintaining accuracy in line with document(s) content.\n",
        "6.Summarize pertinent insights from the document(s) pertinent to the instruction.\n",
        "7.Include references citing PDF names and page numbers for relevant excerpts supporting your response.\n",
        "8.Tailor your response to the instruction's nature, concentrating on precision and pertinence.\n",
        "\n",
        "Context:{context}\n",
        "Instruction:{instruction}\n",
        "\"\"\"\n",
        "    return _prompt\n"
      ],
      "metadata": {
        "id": "Ci0hUaAchXMH"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fin_prompt = qa_prompt(query, context)\n",
        "generate_answer(fin_prompt,openai_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "E5Ij31Fvhj-2",
        "outputId": "55a3a6be-064e-424b-9095-abb5acf87a93"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The lipreading datasets mentioned in the provided document excerpts are:\\n\\n1. AVICar\\n2. AVLetters\\n3. AVLetters2\\n4. BBC TV\\n5. CUAVE\\n6. OuluVS1\\n7. OuluVS2\\n8. GRID corpus\\n\\nThese datasets are used for training and evaluating lipreading models, with the GRID corpus being highlighted as one containing a significant number of sentences (34 speakers producing 1000 sentences each) and used for state-of-the-art performance summaries in lipreading tasks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oFJ3xEaIhttM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}